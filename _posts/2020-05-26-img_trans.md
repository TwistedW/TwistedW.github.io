---
layout: post
title: 近年图像翻译先进模型小结
category: 技术
tags: [GAN]
description: 
---

> 计算机视觉下的任务纷繁庞大，除了熟悉的目标检测、图像识别、图像分类等常见的视觉应用，还有着图像翻译、图像分割和图像超分辨率等十分具有研究和应用价值的方向。本文就近年（2019和2020年）图像翻译下的先进模型进行典型性介绍，一起梳理下图像翻译的发展和未来研究的趋势。

# 图像翻译的发展

图像翻译旨在通过设计端到端的模型将源域图像转换到目标域图像，通常源域提供图像的内容，目标域提供图像的“风格”(可以是图像属性或图像风格)，在源域内容下实现目标域的“风格”化，从而实现源域图像到目标域图像的转换。说的通俗点图像翻译可以是标签图到场景图的转换、线条轮廓到色彩图像转换、图像的风格转换，春夏场景的变换，人脸的属性变换，也可以是白昼交替的转换。只要符合上述端到端转换的任务，都可以通过图像翻译实现。引用[pix2pix](https://arxiv.org/abs/1611.07004)[1]中的经典插图，一起看下图像翻译的实际应用。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5xvk9zbxj31jj0leqv5.jpg">
</p>

<p align="center">
    图1.图像翻译的不同任务场景
</p>

图像翻译自深度学习应用下便得到了快速的发展，尤其是随着生成对抗网络（GAN）的提出，大大加速了图像翻译的演进。从早期的pix2pix、[CycleGAN](https://arxiv.org/abs/1703.10593)[2]、[UNIT](https://arxiv.org/abs/1703.00848)[3]到较为成熟的[StarGAN](https://arxiv.org/abs/1711.09020)[4]都是图像翻译在上几年较为成功和经典的模型。这些模型实现了从源域图像到目标域图像的转换，但这往往需要一定的标签参与或者需要建立源域和目标域各自的生成器，同时任务往往实现的是单目标域的转换。随着发展到[MUNIT](https://arxiv.org/abs/1804.04732)[5]、[DRIT](https://arxiv.org/abs/1905.01270)[6]以及[UGATIT](https://arxiv.org/abs/1907.10830)[7]则进一步实现了由源域到多目标域的转换，也有利用语义mask图像实现无条件图像翻译的[SPADE](https://arxiv.org/abs/1903.07291)[8]。

[StyleGAN](https://arxiv.org/abs/1812.04948)[9]实现了高质量的图像风格转换，这无疑于StyleGAN的细致的架构，逐步分辨率的阶段性生成、自适应实例正则化（AdaIN）和风格空间的应用，[StyleGAN2](https://arxiv.org/abs/1912.04958)[10]在StyleGAN的基础上进一步对AdnIN进行修正，demodulation操作应用于每个卷积层相关的权重，并且通过skip generator代替progressive growing，实现了更为细致的图像转换。这些基础性的图像转换架构对于近年来的图像翻译任务提供价值性的指导。

近年来，图像翻译实现了更加细致的任务实现，[StarGAN v2](https://arxiv.org/abs/1912.01865)[11]在StarGAN的基础上实现了多源域到多目标域的图像转换；[ALAE](https://arxiv.org/abs/2004.04467)[12]将自编码器拓展到高精致的图像转换；[HiDT](https://arxiv.org/abs/2003.08791)[13]提供了多域图像转换下对图像翻译下的网络逻辑和损失函数做了细致的总结；[ConSinGAN](https://arxiv.org/abs/2003.11512)[14]代表了一众单幅图像训练的先进模型，实现了单幅图像训练下的图像转换任务。本文将以这4篇论文进行介绍，对近年图像翻译模型进行分析和小结。

# 图像翻译模型

## StarGAN v2

StarGAN v2针对StarGAN存在的仅能在单目标域下转换和需要标签信息参与的两个问题，提出了目标域下多风格图像的转换。如何实现多目标域的转换呢？StarGAN v2设计了Mapping Network用于生成风格编码，从而为目标域下提供多类型的风格表示而不需要额外的标签，模型的整体结构如图2所示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5xx0ub8xj30ns0c2n0n.jpg" height = "300">
</p>

<p align="center">
    图2.StarGAN v2模型整体结构
</p>

可以看出StarGAN v2由四部分组成，生成器$G$，映射网络$F$，风格编码器$E$判别器$D$。我们先捋一下整个过程，首先映射网络学习到目标域图像的风格编码$\hat s = F_{\hat y}(z)$，其中$\hat y \in \mathcal Y$，这是映射网络学习到的目标域图像的风格编码，而作为参照真实目标域图像的风格编码由风格编码器得到$s=E_y(y)$，得到了风格编码$\hat s$结合源域输入图像$x$便可送入到生成器，生成器输出的就是转换后的目标域图像$G(x, \hat s)$，而判别器则为了区分生成的目标域图像是否是真实来源于真实目标域。

StarGAN v2中映射网络、风格编码器和判别器的输出都是多分支的，因为文章的目的是进行多目标域的转换，这里的多分支就是不同的目标域的表示，对于映射网络和风格编码器，多分支表示的是多个目标域图像的风格表示，对于判别器多分支则是代表不同目标域的判别真假情况，作者在附录中用$N$表示分支数。

图3展示了StarGAN v2的较为详细的网络设计。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5xxpt6faj30ja0iqjvh.jpg" height = "600">
</p>

<p align="center">
    图3.StarGAN v2模型设计结构
</p>

至于损失函数上，值得一提的是风格表示上的风格重构损失$$\mathcal L_{sty} = \mathbb E_{x, \hat{y}, z} [\Vert \hat{s} - E_{\hat{y}}(G(x, \hat{s})) \Vert_1]$$和映射网络丰富化的$$\mathcal L_{ds} = \mathbb E_{x, \hat{y}, z_1, z_2}[\Vert G(x,\hat{s}_1) - G(x, \hat{s}_2) \Vert]$$（使得目标风格表示之间边界分明，产生多目标转换结果，最大化$$\mathcal L_{ds}$$），还有就是较为熟悉的对抗损失$$\mathcal L_{adv}$$和循环一致损失$$\mathcal L_{cyc}$$，在最终的损失优化上：

$$
\mathcal L_D = - \mathcal L_{adv}
$$

$$
\mathcal L_{F,G,E} = \mathcal L_{adv} + \lambda_{sty} \mathcal L_{sty} - \lambda_{ds} \mathcal L_{ds} + \lambda_{cyc} \mathcal L_{cyc}
$$

实验上，在图像转换上展示了优越的效果。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5yatonkgj31ai0r2hdu.jpg" height="350">
</p>

<p align="center">
    图4.StarGAN v2定性对比结果
</p>

## ALAE

自编码器能否具有像GAN那样的生成能力呢？ALAE给出了肯定的答案，ALAE算是建立在StyleGAN基础之上，具备与 GAN 相当的生成能力，且能够学习解耦表征，在人脸属性变换上展示了优越的效果。

ALAE采用自编码器架构，将GAN中的生成器和判别器分解为两个网络，生成器对应着$G$和$F$，判别器对应着$D$和$E$，先从整体架构来看一下ALAE模型。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5yonfscqj30p60a6dhi.jpg" height="250">
</p>

<p align="center">
    图5.ALAE模型结构
</p>

在训练阶段，随机噪声$z$经过网络$F$，将随机噪声映射到数据的潜在空间$w$，网络$G$则负责将数据潜在空间$w$和风格变量$\eta$映射到数据空间。此时输入的$z$经过$F$和$G$得到图像输出$G(w, \eta)$，判别部分由网络$E$和$D$组成，网络$E$将数据编码到潜在空间$q_E(w)$并且与$z$投影得到的$p_F(w)$做分布拉近，理想状态下$q_E(w) = p_F(w)$。

网络$F$是个新颖的设计，它并没有严格的目的性也就是并没有强调和约束数据的潜在空间分布，而是由全局优化下自动学习数据的潜在空间。也正因为存在了数据的潜在空间的刻画，ALAE才可以说是建立在自编码器下结构下的模型。

在测试阶段，由网络$E$和$G$构成了自编码器的编码器和解码器，构成Encoder-Decoder的结构，由$\eta$可以控制图像属性从而实现图像翻译任务。

损失设计上除了对抗损失，还有就是潜在空间分布下的拉近$\min_{E,G}\Delta(F \Vert E \circ G \circ F)$，这也是文章称为为对抗性潜在自动编码器（ALAE）的原因。ALAE在设计上借鉴了StyleGAN的结构并称之为StyleALAE，整体的架构如图6所示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y1yyyzqj30fs0dkac1.jpg" height = "380">
</p>

<p align="center">
    图6.StyleALAE网络架构
</p>

作者使用 MNIST 数据集训练 ALAE，并使用特征表示来执行分类、重建和分析解耦能力的任务，与已有的自编码器模型对比结果存在优势，ALAE最让人印象深刻的就是StyleALAE在FFHQ上的生成效果，真是将自编码器做到了GAN的高精度生成。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5yr0u5d8j30fk08w45v.jpg" height = "300">
</p>

<p align="center">
    图7.StyleALAE的生成效果
</p>

本文围绕着图像翻译展开，ALAE当然适用于图像翻译任务，在人脸属性的转换上也做到了优越的转换效果。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y2wqgakj30v60kkb29.jpg" height = "300">
</p>

<p align="center">
    图8.StyleALAE人脸属性变换
</p>

## ConSinGAN

近年有部分研究者将目光投入到单幅图像训练网络上，ICCV 2019 best paper [ SinGAN](https://arxiv.org/abs/1905.01164)[15]便是一个代表作，此处要介绍的[ConSinGAN]()则是在SinGAN的基础上的升级版。

受限于数据样本和网络训练时长，单幅图像训练的模型存在着很大的应用意义。要介绍清ConSinGAN则必须要提一下SinGAN，本质上ConSinGAN就是并行的SinGAN，缩写中不同的Con就是指Concurrent的意思。

SinGAN按照不同分辨率分阶段训练生成器和判别器，在串行的阶段训练上，当前生成器将前一个生成器生成的图像作为输入，在此基础上生成比当前还要高分辨率的图像，此时不同阶段的生成器都是单独训练的，这也意味着在训练当前生成器时，之前的生成器的权重都保持不变，这个过程由图9所示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y3e4z4bj30os0hy45s.jpg" height = "300">
</p>

<p align="center">
    图9.SinGAN训练过程
</p>

ConSinGAN指出每个分辨率下仅训练一个生成器而固定前面的生成器的参数，这仅仅将前一阶段生成器输出作为当前的输入，这一定程度上限制了不同阶段生成器之间的交互。ConSInGAN设计了对生成器进行端到端的训练，也就是说，在给定时间内可以训练多个生成器，每个生成器将前一个生成器生成的特征(而不是图像)作为输入。这种训练是对多个阶段下的生成器同时进行的，也称之为并行训练的方式，这个过程如图10所示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y3r8tm7j30li0oan5k.jpg" height = "500">
</p>

<p align="center">
    图10.ConSinGAN训练过程
</p>

然而训练多个分辨率下的生成器将会导致另一个问题，那就是过拟合，也就是最后得到的图像失去了多样性，为了解决这个问题，ConSinGAN提出了2个应对方案。

- 在任意给定时间内，只训练一部分生成器
- 在训练一部分生成器时，还要对不同的生成器使用不同的学习率，对于低分辨率阶段的生成器使用较小的学习率

文章和源码中默认最多同时训练3个生成器，此时对前两阶段的生成器采用当前学习率的$\frac{1}{10}$和$\frac{1}{100}$，这个过程图11进行展示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y45jegcj30u01nvgul.jpg" height = "900">
</p>

<p align="center">
    图11.ConSinGAN训练不同生成器不同学习率
</p>

实验发现如果对早阶段的生成器采用较高的学习率，那么生成的图像质量会高些，但是差异性较弱。相反，如果对早阶段的生成器采用较小的学习率，那么生成图像的差异性会丰富一些。

在进行图像翻译任务时，ConSinGAN进行了图像协调实验，主要与SinGAN进行对比，得到的对比结果如图12所示。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y4nkdn9j31is0u0e83.jpg" height = "350">
</p>

<p align="center">
    图12.ConSinGAN在图像协调下的转换结果
</p>

## HiDT

最后来分析下[High-Resolution Daytime Translation Without Domain Labels（HiDT）](https://arxiv.org/abs/2003.08791)，这篇文章虽然做的是高清自然场景时移变换，但是确实将图像翻译的训练逻辑和损失函数介绍的非常清晰的文章，HiDT也是CVPR 2020 oral的一篇文章，在这里梳理一下HiDT对图像翻译的设计逻辑和损失函数的设计。

<p align="center">
    <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gf5y51i85xj30um0noq6f.jpg" height = "300">
</p>

<p align="center">
    图13.HiDT网络优化过程
</p>

先交代下符号，$x$表示源域输入图像，$E_c$表示内容编码器相对应的$c$为内容编码，$E_s$表示风格编码器相对应的$s$为风格编码，$G$为生成器，$s^\prime$为目标域风格编码，$p^*(s)$为风格编码的先验分布，$s_r$为在风格编码的先验分布下随机采样的风格编码。生成器$G$不光光输出的是转换后的图像，同时也输出相对应的风格掩码图$m$。从上到下分析，随机风格采样$s_r$与内容编码$c$生成$G(c,s_r)=x_r,m_r$，此时$x_r$的风格取决于随机风格$s_r$，掩码$m_r$则是受内容$c$的影响，对$x_r$继续进行内容和风格编码得到$$\tilde{c}_r,\tilde{s}_r$，将$\tilde{c}_r,\tilde{s}_r$$馈送到生成器$$G$$得到重构的$$\tilde{x}_r$$，为什么说是重构呢？因为此时输入的风格是$$x_r$$自身的风格编码；中间一路就是对$$x$$进行编码后再重构得到$$\tilde{x}$$；最下面一路则是先根据源域内容编码$c$与目标域风格编码$$s^\prime$$生成得到目标域图像和分割掩码$$\hat{x},\hat{m}$$，再由$$\hat{x}$$编码得到的内容编码$$\hat{c}$$与风格编码$$\hat{s}^\prime$$得到最原始源域图像$$\tilde{\hat{x}}$$，由于$$\tilde{\hat{x}}$$给出的损失为$$\mathcal L_{cyc}$$，这里推测风格编码$$\hat{s}^\prime$$就是源域图像的风格表示。

上述分析，总结起来就是模型在优化阶段由三种模式，一是随机风格的转换和重构，二是原始图像的重构，三是目标域图像转换和循环一致的转换。

最后就是如何对模型进行损失优化，正由图13中所展示的，重构损失$\mathcal L_{rec}$，风格掩码损失$\mathcal L_{seg}$，内容编码损失$\mathcal L_{c}$，风格编码损失$\mathcal L_s$，风格编码下趋紧先验分布的损失$\mathcal L_{dist}$以及循环一致损失$\mathcal L_{cyc}$，由此衍生的$\mathcal L_{seg}^r , \mathcal L_c^r, \mathcal L_s^r, \mathcal L_{rec}^r$也是一样的含义，图13中省略了对抗损失$\mathcal L_{adv}, \mathcal L_{adv}^r$，对抗损失主要是对转换后的$\hat x$和$x_r$进行优化。

重构损失为$L_1$损失，即$\mathcal L_{rec}=\Vert \tilde x - x \Vert_1$，类似的有$\mathcal L_{rec}^r = \Vert \tilde x_r - x_r \Vert_1$，循环一致损失也是采用$L_1$损失$\mathcal L_{cyc} = \Vert \tilde{\hat{x}} - x \Vert_1$。对于分割掩码损失则是采取交叉熵损失$CE(m,\hat m)=-\sum_{i,j}m_{i,j} \log \hat m_{i,j}$，则有$\mathcal L_{seg}=CE(m, \hat{m}), L_{seg}^r=CE(m, m_r)$。由于风格编码的维度较低，此时可以通过均值和方差拉向正态分布，达到风格编码向先验分布靠近，$\mathcal L_{dist} = \Vert \hat\mu_r \Vert_1 + \Vert \hat\sum_T - I \Vert_1 + \Vert diag(\hat\sum_T) - 1 \Vert_1$。对于内容编码损失$\mathcal L_{c}$和风格编码损失$\mathcal L_s$，则是通过$L_1$损失一致性优化，即$\mathcal L_s = \Vert \hat s - s^\prime \Vert_1, \mathcal L_s^r = \Vert \tilde s - s_r \Vert_1, \mathcal L_c = \Vert \hat c - c \Vert_1, \mathcal L_c^r = \Vert \tilde{c}_r - c \Vert_1$。

总的损失可变式为：

$$
\begin{equation}
\begin{aligned}
\min_{E_c,E_s,G}\max_D \mathcal L(E_c,E_s,G,D) &= \lambda_1(\mathcal L_{adv} + \mathcal L_{adv}^r)+\lambda_2(\mathcal L_{rec}+ \mathcal L_{rec}^r+ \mathcal L_{cyc}) +\lambda_3(\mathcal L_{seg} + \mathcal L_{seg}^r) \\
&+\lambda_4(\mathcal L_{c} + \mathcal L_{c}^r) +\lambda_5 \mathcal L_{s}+\lambda_6 \mathcal L_{s}^r +\lambda_7 \mathcal L_{dist}
\end{aligned}
\end{equation}
$$

其中$\lambda_{i},i \in [1,7]$为超参数。HiDT的模型优化算是对图像翻译下的损失进行了一个系统的介绍，同时也是我认为在不系统阅读代码下对图像翻译下的逻辑介绍最为清晰的一篇文章。

# 总结

近年来图像翻译的文章还有很多，本文仅仅是笔者选摘的有代表性的几篇文章，图像翻译已不再是简单的图像风格变换或是源域到目标域的转换，而是上升到多源域到多目标域图像的转换。同时基于GAN，基于自编码器，基于pixelCNN的模型也是十分丰富。从庞大的数据集进行训练，到可以由单幅图像的训练，对于训练的样本要求也在逐步降低。

总的来说，图像翻译向着更加系统和全面的方向有序进展，虽然模型做到了多源域到多目标域图像的转换，但是这个过程仍存在很多的限制，限制于数据集和各目标域间的标注，同时不同目标域间的差距仍不能做到很大，one for all的理念仍是一个很值得研究和迈进的方向。

# 参考文献

[1] Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134.

[2] Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232.

[3] Liu M Y, Breuel T, Kautz J. Unsupervised image-to-image translation networks[C]//Advances in neural information processing systems. 2017: 700-708.

[4] Choi Y, Choi M, Kim M, et al. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8789-8797.

[5] Huang X, Liu M Y, Belongie S, et al. Multimodal unsupervised image-to-image translation[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 172-189.

[6] Lee H Y, Tseng H Y, Mao Q, et al. Drit++: Diverse image-to-image translation via disentangled representations[J]. International Journal of Computer Vision, 2020: 1-16.

[7] Kim J, Kim M, Kang H, et al. U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation[J]. arXiv preprint arXiv:1907.10830, 2019.

[8] Park T, Liu M Y, Wang T C, et al. Semantic image synthesis with spatially-adaptive normalization[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 2337-2346.

[9] Karras T, Laine S, Aila T. A style-based generator architecture for generative adversarial networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 4401-4410.

[10] Karras T, Laine S, Aittala M, et al. Analyzing and improving the image quality of stylegan[J]. arXiv preprint arXiv:1912.04958, 2019.

[11] Choi Y, Uh Y, Yoo J, et al. StarGAN v2: Diverse Image Synthesis for Multiple Domains[J]. arXiv preprint arXiv:1912.01865, 2019.

[12] Pidhorskyi, Stanislav and Adjeroh, Donald A and Doretto, Gianfranco, et al. Adversarial Latent Autoencoders[J]. arXiv preprint arXiv:2004.04467, 2020.

[13] Anokhin I, Solovev P, Korzhenkov D, et al. High-Resolution Daytime Translation Without Domain Labels[J]. arXiv preprint arXiv:2003.08791, 2020.

[14] Hinz T, Fisher M, Wang O, et al. Improved Techniques for Training Single-Image GANs[J]. arXiv preprint arXiv:2003.11512, 2020.

[15] Rott Shaham T, Dekel T, Michaeli T. SinGAN: Learning a Generative Model from a Single Natural Image[J]. arXiv preprint arXiv:1905.01164, 2019.
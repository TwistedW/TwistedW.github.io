---
layout: post
title: 梯度下降法
category: 技术
tags: [Machine Learning,Math]
description: 
---

> 梯度下降法已经在神经网络的网络优化上普遍性使用，今天我们一起来研究一下梯度下降法。

# 1.梯度 #

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y
求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)^T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)^T.
或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)^T,以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点
(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)^T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到
函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)^T的方向，梯度减少最快，也就是更加容易找到函数的
最小值。

# 2.梯度下降与梯度上升 #

在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，
如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数f(θ)的最小值，这时我们需要用梯度下降法来迭代求解。但是
实际上，我们可以反过来求解损失函数 -f(θ)的最大值，这时梯度上升法就派上用场了。

下面来详细总结下梯度下降法。

# 3.梯度下降法算法详解 #

**梯度下降的直观解释**

首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是
在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，
向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能
我们不能走到山脚，而是到了某一个局部的山峰低处。

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法
得到的解就一定是全局最优解。
   
![](/assets/img/GD/GD.png)

**梯度下降的相关概念**

在详细了解梯度下降的算法之前，我们先看看相关的一些概念。

1.步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当
前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

2.特征（feature）：指的是样本中输入部分，比如样本（x0,y0）,（x1,y1）,则样本特征为x，样本输出为y。

3.假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)。比如对于样本（xi,yi）
(i=1,2,...n),可以采用拟合函数如下： hθ(x) = θ0+θ1x。
    
4.损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，
对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),
采用线性回归，损失函数为：
    
![](/assets/img/GD/equation1.png)
    
其中xi表示样本特征x的第i个元素，yi表示样本输出y的第i个元素，hθ(xi)为假设函数。
    
# 4.梯度下降的详细算法 #

梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的
简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。
 
**梯度下降法的代数方式描述**

1.先决条件： 确认优化模型的假设函数和损失函数。

比如对于线性回归，假设函数表示为![](/assets/img/GD/equation2.png), 其中θi(i = 0,1,2... n)为模型参数，xi(i = 0,1,2... n)
为每个样本的n个特征值。这个表示可以简化，我们增加一个特征x0=1，这样![](/assets/img/GD/equation3.png)

同样是线性回归，对应于上面的假设函数，损失函数为：

![](/assets/img/GD/equation4.png)

2.算法相关参数初始化：主要是初始化θ0,θ1...,θn,算法终止距离ε以及步长α。在没有任何先验知识的时候，我喜欢将所有的θ
初始化为0， 将步长初始化为1。在调优的时候再优化。

3.算法过程：

1）确定当前位置的损失函数的梯度，对于θi,其梯度表达式如下：

![](/assets/img/GD/equation5.png)

2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即α![](/assets/img/GD/equation5.png)对应于前面登山例
子中的某一步。

3）确定是否所有的θi,梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的θi(i=0,1,...n)即为最终结果。否则进入步骤4.

4）更新所有的θ，对于θi，其更新表达式如下。更新完毕后继续转入步骤1.

![](/assets/img/GD/equation6.png)

下面用线性回归的例子来具体描述梯度下降。假设我们的样本是

![](/assets/img/GD/equation7.png)损失函数如前面先决条件所述：

![](/assets/img/GD/equation8.png)

则在算法过程步骤1中对于θi的偏导数计算如下：

![](/assets/img/GD/equation9.png)

由于样本中没有x0上式中令所有的![](/assets/img/GD/equation10.png)为1.

步骤4中θi的更新表达式如下：

![](/assets/img/GD/equation11.png)

从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加1/m 是为了好理解。由于步长也为常数，他们的乘机也为常数，所以
这里α1/m可以用一个常数表示。

强调一下上述的算法描述可以转换为矩阵形式，具体转换看[这里](https://www.cnblogs.com/pinard/p/5970503.html)

**梯度下降的算法调优**

在使用梯度下降时，需要进行调优。哪些地方需要调优呢？

1.算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行
算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过
最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。

2.算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸
函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每
个特征x，求出它的期望E(x)和标准差std(x)，然后转化为：

![](/assets/img/GD/equation12.png)

这样特征的新期望为0，新方差为1，迭代次数可以大大加快。

# 5.梯度下降法大家族(BGD，SGD，MBGD) #

**批量梯度下降法（Batch Gradient Descent）**
批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于前面的线性
回归的梯度下降算法，也就是说上述的梯度下降算法就是批量梯度下降法。

![](/assets/img/GD/equation13.png)

由于我们有m个样本，这里求梯度的时候就用了所有m个样本的梯度数据。

**随机梯度下降法（Stochastic Gradient Descent）**
随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。
对应的更新公式是：

![](/assets/img/GD/equation14.png)

随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常
突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，
训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度
来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是接下来要说的小批量梯度下降法。

**小批量梯度下降法（Mini-batch Gradient Descent）**
小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，
当然根据样本的数据，可以调整这个x的值。对应的更新公式是：

![](/assets/img/GD/equation15.png)

# 6.梯度下降法和其他无约束优化算法的比较 #

在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。

梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。
如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于
需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵
或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

谢谢观看，希望对您有所帮助，欢迎指正错误，欢迎一起讨论！！！



